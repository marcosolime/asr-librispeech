{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground ⛹️\n",
    "- a \"battlefield\" - a place to experiment and test things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset, augment data, print samples, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.preprocessing import Preprocessing\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\.conda\\envs\\exp\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import custom preprocessing library\n",
    "prep = Preprocessing()\n",
    "train_set = prep.download(split='train', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------ Printing info for sample [22126] ------+\n",
      "- Waveform is a Tensor of size [1, 229440], type=torch.float32\n",
      "- Sample rate: 16000\n",
      "- Transcript: TALKED ABOUT BUT NOT IN A HARMFUL WAY AND ...\n",
      "- Speaker id: 7067\n",
      "- Chapter id: 76047\n",
      "- Utterance id: 27\n",
      "+---------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Print random audio sample\n",
    "prep.print_raw_sample(np.random.randint(len(train_set)), train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: prep.preprocess(x, \"train\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------ Dataloader length: 28539 ------+\n",
      "# Batches: 1784\n",
      "Spectogram shape: [16, 1, 128, 1349]\n",
      "Label shape: [16, 274]\n",
      "Mel length (length of each spectogram): [603, 549, 493, 602, 519, 598] ...\n",
      "Idx length (length of each label): [274, 177, 225, 215, 211, 210] ...\n",
      "+------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Inspect batch of data\n",
    "prep.print_loader_info(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom models and neural layers, and test them on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.deep_speech_2 import MyLayerNorm, MySkipCNN, MyBiGRU, MyDeepSpeech\n",
    "from torch.nn import Conv2d, Linear\n",
    "\n",
    "#layer_norm = MyLayerNorm(n_bins=128)\n",
    "cnn = Conv2d(1, 32, 3, stride=2, padding=1)\n",
    "skip_cnn = MySkipCNN(32, 32, kernel=3, stride=1, drop_rate=0.5, n_bins=64)\n",
    "fc = Linear(2048, 512)\n",
    "\n",
    "bigru_1 = MyBiGRU(512, 512, 0.5, True)\n",
    "bigru_2 = MyBiGRU(1024, 512, 0.5, True)\n",
    "\n",
    "deep_speech = MyDeepSpeech(3, 5, 512, 29, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: torch.Size([16, 1, 128, 1265])\n",
      "Shape after CNN and SkipCNN: torch.Size([16, 32, 64, 633])\n",
      "Shape after view: torch.Size([16, 2048, 633])\n",
      "Shape after tranpose: torch.Size([16, 633, 2048])\n",
      "Shape after linear layer: torch.Size([16, 633, 512])\n",
      "Shape after bigru 1: torch.Size([16, 633, 1024])\n",
      "Shape after bigru 2: torch.Size([16, 633, 1024])\n",
      "Shape after final linear classifier: torch.Size([16, 633, 27])\n"
     ]
    }
   ],
   "source": [
    "# Check shapes layer by layer\n",
    "for idx, data in enumerate(train_loader):\n",
    "    \n",
    "    # batch of data to play with\n",
    "    spec = data[0]\n",
    "\n",
    "    #### layer norm ####\n",
    "    # print(f\"Shape before: {spec.shape}\")\n",
    "    # out = layer_norm(spec)\n",
    "    # print(f\"Shape after: {out.shape}\")\n",
    "    \n",
    "    # Skip cnn\n",
    "    print(f\"Shape before: {spec.shape}\") # [16, 1, 128, 1315]\n",
    "    out = cnn(spec)\n",
    "    out = skip_cnn(out)\n",
    "\n",
    "    sizes = out.shape\n",
    "    print(f\"Shape after CNN and SkipCNN: {sizes}\") # [16, 32, 64, 658]\n",
    "    \n",
    "    out = out.view(sizes[0], sizes[1]*sizes[2], sizes[3])\n",
    "    print(f\"Shape after view: {out.shape}\") # [16, 2048, 658]\n",
    "\n",
    "    out = out.transpose(1, 2)\n",
    "    print(f\"Shape after tranpose: {out.shape}\") # [16, 658, 2048]\n",
    "\n",
    "    out = fc(out)\n",
    "    print(f\"Shape after linear layer: {out.shape}\") # [16, 648, 512]\n",
    "\n",
    "    out = bigru_1(out)\n",
    "    print(f\"Shape after bigru 1: {out.shape}\")  # [16, 648, 1024]\n",
    "    \n",
    "    out = bigru_2(out)\n",
    "    print(f\"Shape after bigru 2: {out.shape}\")  # [16, 648, 1024]\n",
    "\n",
    "    out = Linear(1024, 512)(out)\n",
    "    out = nn.GELU()(out)\n",
    "    out = nn.Dropout(0.5)(out)\n",
    "    out = nn.Linear(512, 27)(out)\n",
    "    print(f\"Shape after final linear classifier: {out.shape}\") # [16, 639, 27]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 1, 128, 1305])\n",
      "Deep: torch.Size([16, 32, 64, 653])\n",
      "Deep: torch.Size([16, 32, 64, 653])\n",
      "Deep: torch.Size([16, 653, 512])\n",
      "After RNN block: torch.Size([16, 653, 1024])\n",
      "Final: torch.Size([16, 653, 29])\n",
      "Output shape: torch.Size([16, 653, 29])\n"
     ]
    }
   ],
   "source": [
    "# Check deep speech model\n",
    "for idx, data in enumerate(train_loader):\n",
    "    spec = data[0]\n",
    "\n",
    "    print(f\"Input shape: {spec.shape}\")\n",
    "    out = deep_speech(spec)\n",
    "    print(f\"Output shape: {out.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 29 10:28:08 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.70                 Driver Version: 537.70       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   26C    P0              23W / 105W |      0MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marco\\.conda\\envs\\exp\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:42:31) [MSC v.1937 64 bit (AMD64)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandb - dummy script to log accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1305 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (0.16.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (3.1.42)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (1.43.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (69.2.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\marco\\.conda\\envs\\nlp_env\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\marco\\.netrc\n"
     ]
    }
   ],
   "source": [
    "! wandb login 33a194fab1b28225adfa0561f9a0dcbae4adff8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miu4hry\u001b[0m (\u001b[33mfantastic_4\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\marco\\OneDrive - Alma Mater Studiorum Università di Bologna\\University\\NLP_Torroni\\ASR\\app\\wandb\\run-20240326_193230-jlp2ekhy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fantastic_4/asr_librispeech/runs/jlp2ekhy' target=\"_blank\">devoted-field-6</a></strong> to <a href='https://wandb.ai/fantastic_4/asr_librispeech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fantastic_4/asr_librispeech' target=\"_blank\">https://wandb.ai/fantastic_4/asr_librispeech</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fantastic_4/asr_librispeech/runs/jlp2ekhy' target=\"_blank\">https://wandb.ai/fantastic_4/asr_librispeech/runs/jlp2ekhy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▇▇█▇█▇█████████████████████████████████</td></tr><tr><td>loss</td><td>█▃▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.86983</td></tr><tr><td>loss</td><td>0.12401</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-field-6</strong> at: <a href='https://wandb.ai/fantastic_4/asr_librispeech/runs/jlp2ekhy' target=\"_blank\">https://wandb.ai/fantastic_4/asr_librispeech/runs/jlp2ekhy</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240326_193230-jlp2ekhy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# Init\n",
    "wandb.init(\n",
    "    project=\"asr_librispeech\",\n",
    "\n",
    "    # hyperparams to track\n",
    "    config= {\n",
    "        \"lr\": 0.001,\n",
    "        \"model\": \"dummy_number_1\",\n",
    "        \"dataset\": \"db_fool\",\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train\n",
    "epochs = 100\n",
    "offset = random.random() / 5\n",
    "acc = 0\n",
    "for epoch in range(2, epochs):\n",
    "\n",
    "    # compute dummy accuracy and loss\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    #acc += 0.4\n",
    "    #acc = acc % 1\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # LOG!\n",
    "    wandb.log({\n",
    "        \"acc\": acc,\n",
    "        \"loss\": loss\n",
    "    })\n",
    "\n",
    "# Stop all the thing (put this line only in notebooks)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import word_errors, wer, cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_errors(\"harry potter\", \"harry osborne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter operator - removes any empty strings resulting from consecutive white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n",
      "['harry', 'potter', 'is', 'a', '', 'wizard']\n",
      "\n",
      "WITHOUT FILTER\n",
      "harry potter is a  wizard\n",
      "\n",
      "WITH FILTER\n",
      "harry potter is a wizard\n",
      "\n",
      "COLLAPSE MODE\n",
      "harrypotterisawizard\n"
     ]
    }
   ],
   "source": [
    "reference = \"harry potter is a  wizard\"\n",
    "print('ORIGINAL')\n",
    "print(reference.split(' '))\n",
    "print()\n",
    "\n",
    "print('WITHOUT FILTER')\n",
    "print(\" \".join(reference.split(' ')))\n",
    "print()\n",
    "\n",
    "print('WITH FILTER')\n",
    "print(\" \".join(filter(None, reference.split(' '))))\n",
    "print()\n",
    "\n",
    "print('COLLAPSE MODE')\n",
    "print(\"\".join(filter(None, reference.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(\"harry potter is a magician\", \"harry potter is a wizard\") # 1/5 = 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(\"Peter Parker is Spider\", \"peter park is spider\") # 1 errors over 4 -> 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between CER and WER\n",
    "- CER: works at char level. Basically, it is a huge edit distance between ref and hyp. May be a good idea to test our models.\n",
    "- WER: computes the edit distance at word level. In some cases, we get drastic chages after deleting one space - see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER: 0.1\n",
      "WER 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"CER:\", cer(\"spider man\", \"spiderman\")) # 1/10 = 0.1 GOOD\n",
    "print(\"WER\", wer(\"spider man\", \"spiderman\")) # 2/2 = 1.0 BAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTC Loss. So basically, once we have the model predictions we need to take the log of the softmax. Why? Because it is written on the doc. Also, the shape is a bit silly - `[time, batch, classes]`. Targets must be [batch, length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.full(size=(16,), fill_value=50, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around with Bidirectional GRU blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: torch.Size([16, 650, 512])\n",
      "After: torch.Size([16, 650, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.blocks import BiGRU\n",
    "\n",
    "seq_len, emb_size = 650, 512\n",
    "hid_size = 32\n",
    "input_size = (16, seq_len, emb_size)\n",
    "\n",
    "gru = BiGRU(emb_size, hid_size, 0.1, False)\n",
    "\n",
    "x = torch.randn(input_size)\n",
    "\n",
    "print(f\"Before: {x.shape}\")\n",
    "\n",
    "x = gru(x)\n",
    "\n",
    "print(f\"After: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 650, 29])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.deep_speech import DeepSpeech\n",
    "import torch\n",
    "\n",
    "x = torch.randn((16, 1, 128, 1300))\n",
    "\n",
    "dp = DeepSpeech(3, 2, 512, 29, 128)\n",
    "\n",
    "out = dp(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"abcdefghijklmnopqrstuvwxyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing aroung with attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 650, 650])\n"
     ]
    }
   ],
   "source": [
    "input_shape = (1, 650, 512)\n",
    "features = torch.randn((input_shape))\n",
    "\n",
    "multi_head_attention = nn.MultiheadAttention(\n",
    "    embed_dim=input_shape[2],\n",
    "    num_heads=4,\n",
    "    batch_first=True)\n",
    "\n",
    "attn_output, att_weights = multi_head_attention(features, features, features)\n",
    "print(att_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 650, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (1, 650, 512)\n",
    "features = torch.randn((input_shape))\n",
    "\n",
    "layer_norm = nn.LayerNorm(512)\n",
    "out = layer_norm(features)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIB WER: 1.0357142857142858\n",
      "MY WER:  1.7142857142857142\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from utils.metrics import word_errors \n",
    "\n",
    "def compute_wer(reference, hypothesis):\n",
    "    # Split sentences into words\n",
    "    reference_words = reference.split()\n",
    "    hypothesis_words = hypothesis.split()\n",
    "    \n",
    "    # Compute Levenshtein distance\n",
    "    distance = Levenshtein.distance(reference_words, hypothesis_words)\n",
    "    \n",
    "    # Normalize distance by the number of words in reference sentence\n",
    "    wer = distance / len(reference_words)\n",
    "    \n",
    "    return wer\n",
    "\n",
    "# Example usage\n",
    "reference_sentence = 'he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce'\n",
    "hypothesis_sentence = 'hhe  hhoppedd thhey wwould  bee  ssto  ferr dinnerrr  turrnnnipss andd  carrrrritts endd  broseed  betaehose annd  tatt  muttenn ppeccess  too  bbe   latlld ouutt inn to  thhicck  ppeperrred   floerr  fatenn  soss'\n",
    "wer_lib = compute_wer(reference_sentence, hypothesis_sentence)\n",
    "wer_my = word_errors(reference_sentence, hypothesis_sentence)\n",
    "\n",
    "print(\"LIB WER:\", wer_lib)\n",
    "print(\"MY WER: \", wer_my[0]/wer_my[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cer 0.0\n",
      "wer 0.0\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "def compute_wer(hyp, ref):\n",
    "    hyp_words = hyp.split()\n",
    "    ref_words = ref.split()\n",
    "\n",
    "    wer = Levenshtein.distance(hyp_words, ref_words) / len(ref_words)\n",
    "    return wer\n",
    "\n",
    "def compute_cer(hyp, ref):\n",
    "    cer = Levenshtein.distance(hyp, ref) / len(ref)\n",
    "    return cer\n",
    "\n",
    "ref = \"he hoped there would be stew\"\n",
    "hyp = \"he hoped there would be stew\"\n",
    "\n",
    "hyp = hyp.lower()\n",
    "ref = ref.lower()\n",
    "\n",
    "wer = compute_wer(hyp, ref)\n",
    "cer = compute_cer(hyp, ref)\n",
    "\n",
    "print('cer', cer)\n",
    "print('wer', wer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out the Pre-norm encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.blocks import PrenormEncoder\n",
    "import torch\n",
    "\n",
    "seq_len = 650\n",
    "emb_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "x = torch.randn(32, seq_len, emb_dim)\n",
    "\n",
    "encoder = PrenormEncoder(seq_len, emb_dim, num_heads, drop_rate=0.2)\n",
    "\n",
    "print(f\"{x.shape}\")\n",
    "out = encoder(x)\n",
    "print(f\"{out.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating bi-grams instead of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: [\"'\", '[space]', 'a', 'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'b', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm', 'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz', 'c', 'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz', 'd', 'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm', 'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz', 'e', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez', 'f', 'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm', 'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz', 'g', 'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm', 'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz', 'h', 'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm', 'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz', 'i', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iu', 'iv', 'iw', 'ix', 'iy', 'iz', 'j', 'ja', 'jb', 'jc', 'jd', 'je', 'jf', 'jg', 'jh', 'ji', 'jj', 'jk', 'jl', 'jm', 'jn', 'jo', 'jp', 'jq', 'jr', 'js', 'jt', 'ju', 'jv', 'jw', 'jx', 'jy', 'jz', 'k', 'ka', 'kb', 'kc', 'kd', 'ke', 'kf', 'kg', 'kh', 'ki', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'kp', 'kq', 'kr', 'ks', 'kt', 'ku', 'kv', 'kw', 'kx', 'ky', 'kz', 'l', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'lg', 'lh', 'li', 'lj', 'lk', 'll', 'lm', 'ln', 'lo', 'lp', 'lq', 'lr', 'ls', 'lt', 'lu', 'lv', 'lw', 'lx', 'ly', 'lz', 'm', 'ma', 'mb', 'mc', 'md', 'me', 'mf', 'mg', 'mh', 'mi', 'mj', 'mk', 'ml', 'mm', 'mn', 'mo', 'mp', 'mq', 'mr', 'ms', 'mt', 'mu', 'mv', 'mw', 'mx', 'my', 'mz', 'n', 'na', 'nb', 'nc', 'nd', 'ne', 'nf', 'ng', 'nh', 'ni', 'nj', 'nk', 'nl', 'nm', 'nn', 'no', 'np', 'nq', 'nr', 'ns', 'nt', 'nu', 'nv', 'nw', 'nx', 'ny', 'nz', 'o', 'oa', 'ob', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'oi', 'oj', 'ok', 'ol', 'om', 'on', 'oo', 'op', 'oq', 'or', 'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'oz', 'p', 'pa', 'pb', 'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pp', 'pq', 'pr', 'ps', 'pt', 'pu', 'pv', 'pw', 'px', 'py', 'pz', 'q', 'qa', 'qb', 'qc', 'qd', 'qe', 'qf', 'qg', 'qh', 'qi', 'qj', 'qk', 'ql', 'qm', 'qn', 'qo', 'qp', 'qq', 'qr', 'qs', 'qt', 'qu', 'qv', 'qw', 'qx', 'qy', 'qz', 'r', 'ra', 'rb', 'rc', 'rd', 're', 'rf', 'rg', 'rh', 'ri', 'rj', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rq', 'rr', 'rs', 'rt', 'ru', 'rv', 'rw', 'rx', 'ry', 'rz', 's', 'sa', 'sb', 'sc', 'sd', 'se', 'sf', 'sg', 'sh', 'si', 'sj', 'sk', 'sl', 'sm', 'sn', 'so', 'sp', 'sq', 'sr', 'ss', 'st', 'su', 'sv', 'sw', 'sx', 'sy', 'sz', 't', 'ta', 'tb', 'tc', 'td', 'te', 'tf', 'tg', 'th', 'ti', 'tj', 'tk', 'tl', 'tm', 'tn', 'to', 'tp', 'tq', 'tr', 'ts', 'tt', 'tu', 'tv', 'tw', 'tx', 'ty', 'tz', 'u', 'ua', 'ub', 'uc', 'ud', 'ue', 'uf', 'ug', 'uh', 'ui', 'uj', 'uk', 'ul', 'um', 'un', 'uo', 'up', 'uq', 'ur', 'us', 'ut', 'uu', 'uv', 'uw', 'ux', 'uy', 'uz', 'v', 'va', 'vb', 'vc', 'vd', 've', 'vf', 'vg', 'vh', 'vi', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'w', 'wa', 'wb', 'wc', 'wd', 'we', 'wf', 'wg', 'wh', 'wi', 'wj', 'wk', 'wl', 'wm', 'wn', 'wo', 'wp', 'wq', 'wr', 'ws', 'wt', 'wu', 'wv', 'ww', 'wx', 'wy', 'wz', 'x', 'xa', 'xb', 'xc', 'xd', 'xe', 'xf', 'xg', 'xh', 'xi', 'xj', 'xk', 'xl', 'xm', 'xn', 'xo', 'xp', 'xq', 'xr', 'xs', 'xt', 'xu', 'xv', 'xw', 'xx', 'xy', 'xz', 'y', 'ya', 'yb', 'yc', 'yd', 'ye', 'yf', 'yg', 'yh', 'yi', 'yj', 'yk', 'yl', 'ym', 'yn', 'yo', 'yp', 'yq', 'yr', 'ys', 'yt', 'yu', 'yv', 'yw', 'yx', 'yy', 'yz', 'z', 'za', 'zb', 'zc', 'zd', 'ze', 'zf', 'zg', 'zh', 'zi', 'zj', 'zk', 'zl', 'zm', 'zn', 'zo', 'zp', 'zq', 'zr', 'zs', 'zt', 'zu', 'zv', 'zw', 'zx', 'zy', 'zz']\n",
      "704\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the set of characters\n",
    "characters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "# Generate all possible unigrams and bi-grams from the characters\n",
    "unigrams = list(characters)\n",
    "bi_grams = [''.join(bi_gram) for bi_gram in itertools.product(characters, repeat=2)]\n",
    "\n",
    "# Add space and apostrophe to the vocabulary\n",
    "vocabulary = set(unigrams + bi_grams + ['[space]', \"'\"])\n",
    "\n",
    "# Convert the vocabulary to a list and sort it\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(len(vocabulary))\n",
    "\n",
    "vocabulary = ' '.join(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\": 0,\n",
       " '[space]': 1,\n",
       " 'a': 2,\n",
       " 'aa': 3,\n",
       " 'ab': 4,\n",
       " 'ac': 5,\n",
       " 'ad': 6,\n",
       " 'ae': 7,\n",
       " 'af': 8,\n",
       " 'ag': 9,\n",
       " 'ah': 10,\n",
       " 'ai': 11,\n",
       " 'aj': 12,\n",
       " 'ak': 13,\n",
       " 'al': 14,\n",
       " 'am': 15,\n",
       " 'an': 16,\n",
       " 'ao': 17,\n",
       " 'ap': 18,\n",
       " 'aq': 19,\n",
       " 'ar': 20,\n",
       " 'as': 21,\n",
       " 'at': 22,\n",
       " 'au': 23,\n",
       " 'av': 24,\n",
       " 'aw': 25,\n",
       " 'ax': 26,\n",
       " 'ay': 27,\n",
       " 'az': 28,\n",
       " 'b': 29,\n",
       " 'ba': 30,\n",
       " 'bb': 31,\n",
       " 'bc': 32,\n",
       " 'bd': 33,\n",
       " 'be': 34,\n",
       " 'bf': 35,\n",
       " 'bg': 36,\n",
       " 'bh': 37,\n",
       " 'bi': 38,\n",
       " 'bj': 39,\n",
       " 'bk': 40,\n",
       " 'bl': 41,\n",
       " 'bm': 42,\n",
       " 'bn': 43,\n",
       " 'bo': 44,\n",
       " 'bp': 45,\n",
       " 'bq': 46,\n",
       " 'br': 47,\n",
       " 'bs': 48,\n",
       " 'bt': 49,\n",
       " 'bu': 50,\n",
       " 'bv': 51,\n",
       " 'bw': 52,\n",
       " 'bx': 53,\n",
       " 'by': 54,\n",
       " 'bz': 55,\n",
       " 'c': 56,\n",
       " 'ca': 57,\n",
       " 'cb': 58,\n",
       " 'cc': 59,\n",
       " 'cd': 60,\n",
       " 'ce': 61,\n",
       " 'cf': 62,\n",
       " 'cg': 63,\n",
       " 'ch': 64,\n",
       " 'ci': 65,\n",
       " 'cj': 66,\n",
       " 'ck': 67,\n",
       " 'cl': 68,\n",
       " 'cm': 69,\n",
       " 'cn': 70,\n",
       " 'co': 71,\n",
       " 'cp': 72,\n",
       " 'cq': 73,\n",
       " 'cr': 74,\n",
       " 'cs': 75,\n",
       " 'ct': 76,\n",
       " 'cu': 77,\n",
       " 'cv': 78,\n",
       " 'cw': 79,\n",
       " 'cx': 80,\n",
       " 'cy': 81,\n",
       " 'cz': 82,\n",
       " 'd': 83,\n",
       " 'da': 84,\n",
       " 'db': 85,\n",
       " 'dc': 86,\n",
       " 'dd': 87,\n",
       " 'de': 88,\n",
       " 'df': 89,\n",
       " 'dg': 90,\n",
       " 'dh': 91,\n",
       " 'di': 92,\n",
       " 'dj': 93,\n",
       " 'dk': 94,\n",
       " 'dl': 95,\n",
       " 'dm': 96,\n",
       " 'dn': 97,\n",
       " 'do': 98,\n",
       " 'dp': 99,\n",
       " 'dq': 100,\n",
       " 'dr': 101,\n",
       " 'ds': 102,\n",
       " 'dt': 103,\n",
       " 'du': 104,\n",
       " 'dv': 105,\n",
       " 'dw': 106,\n",
       " 'dx': 107,\n",
       " 'dy': 108,\n",
       " 'dz': 109,\n",
       " 'e': 110,\n",
       " 'ea': 111,\n",
       " 'eb': 112,\n",
       " 'ec': 113,\n",
       " 'ed': 114,\n",
       " 'ee': 115,\n",
       " 'ef': 116,\n",
       " 'eg': 117,\n",
       " 'eh': 118,\n",
       " 'ei': 119,\n",
       " 'ej': 120,\n",
       " 'ek': 121,\n",
       " 'el': 122,\n",
       " 'em': 123,\n",
       " 'en': 124,\n",
       " 'eo': 125,\n",
       " 'ep': 126,\n",
       " 'eq': 127,\n",
       " 'er': 128,\n",
       " 'es': 129,\n",
       " 'et': 130,\n",
       " 'eu': 131,\n",
       " 'ev': 132,\n",
       " 'ew': 133,\n",
       " 'ex': 134,\n",
       " 'ey': 135,\n",
       " 'ez': 136,\n",
       " 'f': 137,\n",
       " 'fa': 138,\n",
       " 'fb': 139,\n",
       " 'fc': 140,\n",
       " 'fd': 141,\n",
       " 'fe': 142,\n",
       " 'ff': 143,\n",
       " 'fg': 144,\n",
       " 'fh': 145,\n",
       " 'fi': 146,\n",
       " 'fj': 147,\n",
       " 'fk': 148,\n",
       " 'fl': 149,\n",
       " 'fm': 150,\n",
       " 'fn': 151,\n",
       " 'fo': 152,\n",
       " 'fp': 153,\n",
       " 'fq': 154,\n",
       " 'fr': 155,\n",
       " 'fs': 156,\n",
       " 'ft': 157,\n",
       " 'fu': 158,\n",
       " 'fv': 159,\n",
       " 'fw': 160,\n",
       " 'fx': 161,\n",
       " 'fy': 162,\n",
       " 'fz': 163,\n",
       " 'g': 164,\n",
       " 'ga': 165,\n",
       " 'gb': 166,\n",
       " 'gc': 167,\n",
       " 'gd': 168,\n",
       " 'ge': 169,\n",
       " 'gf': 170,\n",
       " 'gg': 171,\n",
       " 'gh': 172,\n",
       " 'gi': 173,\n",
       " 'gj': 174,\n",
       " 'gk': 175,\n",
       " 'gl': 176,\n",
       " 'gm': 177,\n",
       " 'gn': 178,\n",
       " 'go': 179,\n",
       " 'gp': 180,\n",
       " 'gq': 181,\n",
       " 'gr': 182,\n",
       " 'gs': 183,\n",
       " 'gt': 184,\n",
       " 'gu': 185,\n",
       " 'gv': 186,\n",
       " 'gw': 187,\n",
       " 'gx': 188,\n",
       " 'gy': 189,\n",
       " 'gz': 190,\n",
       " 'h': 191,\n",
       " 'ha': 192,\n",
       " 'hb': 193,\n",
       " 'hc': 194,\n",
       " 'hd': 195,\n",
       " 'he': 196,\n",
       " 'hf': 197,\n",
       " 'hg': 198,\n",
       " 'hh': 199,\n",
       " 'hi': 200,\n",
       " 'hj': 201,\n",
       " 'hk': 202,\n",
       " 'hl': 203,\n",
       " 'hm': 204,\n",
       " 'hn': 205,\n",
       " 'ho': 206,\n",
       " 'hp': 207,\n",
       " 'hq': 208,\n",
       " 'hr': 209,\n",
       " 'hs': 210,\n",
       " 'ht': 211,\n",
       " 'hu': 212,\n",
       " 'hv': 213,\n",
       " 'hw': 214,\n",
       " 'hx': 215,\n",
       " 'hy': 216,\n",
       " 'hz': 217,\n",
       " 'i': 218,\n",
       " 'ia': 219,\n",
       " 'ib': 220,\n",
       " 'ic': 221,\n",
       " 'id': 222,\n",
       " 'ie': 223,\n",
       " 'if': 224,\n",
       " 'ig': 225,\n",
       " 'ih': 226,\n",
       " 'ii': 227,\n",
       " 'ij': 228,\n",
       " 'ik': 229,\n",
       " 'il': 230,\n",
       " 'im': 231,\n",
       " 'in': 232,\n",
       " 'io': 233,\n",
       " 'ip': 234,\n",
       " 'iq': 235,\n",
       " 'ir': 236,\n",
       " 'is': 237,\n",
       " 'it': 238,\n",
       " 'iu': 239,\n",
       " 'iv': 240,\n",
       " 'iw': 241,\n",
       " 'ix': 242,\n",
       " 'iy': 243,\n",
       " 'iz': 244,\n",
       " 'j': 245,\n",
       " 'ja': 246,\n",
       " 'jb': 247,\n",
       " 'jc': 248,\n",
       " 'jd': 249,\n",
       " 'je': 250,\n",
       " 'jf': 251,\n",
       " 'jg': 252,\n",
       " 'jh': 253,\n",
       " 'ji': 254,\n",
       " 'jj': 255,\n",
       " 'jk': 256,\n",
       " 'jl': 257,\n",
       " 'jm': 258,\n",
       " 'jn': 259,\n",
       " 'jo': 260,\n",
       " 'jp': 261,\n",
       " 'jq': 262,\n",
       " 'jr': 263,\n",
       " 'js': 264,\n",
       " 'jt': 265,\n",
       " 'ju': 266,\n",
       " 'jv': 267,\n",
       " 'jw': 268,\n",
       " 'jx': 269,\n",
       " 'jy': 270,\n",
       " 'jz': 271,\n",
       " 'k': 272,\n",
       " 'ka': 273,\n",
       " 'kb': 274,\n",
       " 'kc': 275,\n",
       " 'kd': 276,\n",
       " 'ke': 277,\n",
       " 'kf': 278,\n",
       " 'kg': 279,\n",
       " 'kh': 280,\n",
       " 'ki': 281,\n",
       " 'kj': 282,\n",
       " 'kk': 283,\n",
       " 'kl': 284,\n",
       " 'km': 285,\n",
       " 'kn': 286,\n",
       " 'ko': 287,\n",
       " 'kp': 288,\n",
       " 'kq': 289,\n",
       " 'kr': 290,\n",
       " 'ks': 291,\n",
       " 'kt': 292,\n",
       " 'ku': 293,\n",
       " 'kv': 294,\n",
       " 'kw': 295,\n",
       " 'kx': 296,\n",
       " 'ky': 297,\n",
       " 'kz': 298,\n",
       " 'l': 299,\n",
       " 'la': 300,\n",
       " 'lb': 301,\n",
       " 'lc': 302,\n",
       " 'ld': 303,\n",
       " 'le': 304,\n",
       " 'lf': 305,\n",
       " 'lg': 306,\n",
       " 'lh': 307,\n",
       " 'li': 308,\n",
       " 'lj': 309,\n",
       " 'lk': 310,\n",
       " 'll': 311,\n",
       " 'lm': 312,\n",
       " 'ln': 313,\n",
       " 'lo': 314,\n",
       " 'lp': 315,\n",
       " 'lq': 316,\n",
       " 'lr': 317,\n",
       " 'ls': 318,\n",
       " 'lt': 319,\n",
       " 'lu': 320,\n",
       " 'lv': 321,\n",
       " 'lw': 322,\n",
       " 'lx': 323,\n",
       " 'ly': 324,\n",
       " 'lz': 325,\n",
       " 'm': 326,\n",
       " 'ma': 327,\n",
       " 'mb': 328,\n",
       " 'mc': 329,\n",
       " 'md': 330,\n",
       " 'me': 331,\n",
       " 'mf': 332,\n",
       " 'mg': 333,\n",
       " 'mh': 334,\n",
       " 'mi': 335,\n",
       " 'mj': 336,\n",
       " 'mk': 337,\n",
       " 'ml': 338,\n",
       " 'mm': 339,\n",
       " 'mn': 340,\n",
       " 'mo': 341,\n",
       " 'mp': 342,\n",
       " 'mq': 343,\n",
       " 'mr': 344,\n",
       " 'ms': 345,\n",
       " 'mt': 346,\n",
       " 'mu': 347,\n",
       " 'mv': 348,\n",
       " 'mw': 349,\n",
       " 'mx': 350,\n",
       " 'my': 351,\n",
       " 'mz': 352,\n",
       " 'n': 353,\n",
       " 'na': 354,\n",
       " 'nb': 355,\n",
       " 'nc': 356,\n",
       " 'nd': 357,\n",
       " 'ne': 358,\n",
       " 'nf': 359,\n",
       " 'ng': 360,\n",
       " 'nh': 361,\n",
       " 'ni': 362,\n",
       " 'nj': 363,\n",
       " 'nk': 364,\n",
       " 'nl': 365,\n",
       " 'nm': 366,\n",
       " 'nn': 367,\n",
       " 'no': 368,\n",
       " 'np': 369,\n",
       " 'nq': 370,\n",
       " 'nr': 371,\n",
       " 'ns': 372,\n",
       " 'nt': 373,\n",
       " 'nu': 374,\n",
       " 'nv': 375,\n",
       " 'nw': 376,\n",
       " 'nx': 377,\n",
       " 'ny': 378,\n",
       " 'nz': 379,\n",
       " 'o': 380,\n",
       " 'oa': 381,\n",
       " 'ob': 382,\n",
       " 'oc': 383,\n",
       " 'od': 384,\n",
       " 'oe': 385,\n",
       " 'of': 386,\n",
       " 'og': 387,\n",
       " 'oh': 388,\n",
       " 'oi': 389,\n",
       " 'oj': 390,\n",
       " 'ok': 391,\n",
       " 'ol': 392,\n",
       " 'om': 393,\n",
       " 'on': 394,\n",
       " 'oo': 395,\n",
       " 'op': 396,\n",
       " 'oq': 397,\n",
       " 'or': 398,\n",
       " 'os': 399,\n",
       " 'ot': 400,\n",
       " 'ou': 401,\n",
       " 'ov': 402,\n",
       " 'ow': 403,\n",
       " 'ox': 404,\n",
       " 'oy': 405,\n",
       " 'oz': 406,\n",
       " 'p': 407,\n",
       " 'pa': 408,\n",
       " 'pb': 409,\n",
       " 'pc': 410,\n",
       " 'pd': 411,\n",
       " 'pe': 412,\n",
       " 'pf': 413,\n",
       " 'pg': 414,\n",
       " 'ph': 415,\n",
       " 'pi': 416,\n",
       " 'pj': 417,\n",
       " 'pk': 418,\n",
       " 'pl': 419,\n",
       " 'pm': 420,\n",
       " 'pn': 421,\n",
       " 'po': 422,\n",
       " 'pp': 423,\n",
       " 'pq': 424,\n",
       " 'pr': 425,\n",
       " 'ps': 426,\n",
       " 'pt': 427,\n",
       " 'pu': 428,\n",
       " 'pv': 429,\n",
       " 'pw': 430,\n",
       " 'px': 431,\n",
       " 'py': 432,\n",
       " 'pz': 433,\n",
       " 'q': 434,\n",
       " 'qa': 435,\n",
       " 'qb': 436,\n",
       " 'qc': 437,\n",
       " 'qd': 438,\n",
       " 'qe': 439,\n",
       " 'qf': 440,\n",
       " 'qg': 441,\n",
       " 'qh': 442,\n",
       " 'qi': 443,\n",
       " 'qj': 444,\n",
       " 'qk': 445,\n",
       " 'ql': 446,\n",
       " 'qm': 447,\n",
       " 'qn': 448,\n",
       " 'qo': 449,\n",
       " 'qp': 450,\n",
       " 'qq': 451,\n",
       " 'qr': 452,\n",
       " 'qs': 453,\n",
       " 'qt': 454,\n",
       " 'qu': 455,\n",
       " 'qv': 456,\n",
       " 'qw': 457,\n",
       " 'qx': 458,\n",
       " 'qy': 459,\n",
       " 'qz': 460,\n",
       " 'r': 461,\n",
       " 'ra': 462,\n",
       " 'rb': 463,\n",
       " 'rc': 464,\n",
       " 'rd': 465,\n",
       " 're': 466,\n",
       " 'rf': 467,\n",
       " 'rg': 468,\n",
       " 'rh': 469,\n",
       " 'ri': 470,\n",
       " 'rj': 471,\n",
       " 'rk': 472,\n",
       " 'rl': 473,\n",
       " 'rm': 474,\n",
       " 'rn': 475,\n",
       " 'ro': 476,\n",
       " 'rp': 477,\n",
       " 'rq': 478,\n",
       " 'rr': 479,\n",
       " 'rs': 480,\n",
       " 'rt': 481,\n",
       " 'ru': 482,\n",
       " 'rv': 483,\n",
       " 'rw': 484,\n",
       " 'rx': 485,\n",
       " 'ry': 486,\n",
       " 'rz': 487,\n",
       " 's': 488,\n",
       " 'sa': 489,\n",
       " 'sb': 490,\n",
       " 'sc': 491,\n",
       " 'sd': 492,\n",
       " 'se': 493,\n",
       " 'sf': 494,\n",
       " 'sg': 495,\n",
       " 'sh': 496,\n",
       " 'si': 497,\n",
       " 'sj': 498,\n",
       " 'sk': 499,\n",
       " 'sl': 500,\n",
       " 'sm': 501,\n",
       " 'sn': 502,\n",
       " 'so': 503,\n",
       " 'sp': 504,\n",
       " 'sq': 505,\n",
       " 'sr': 506,\n",
       " 'ss': 507,\n",
       " 'st': 508,\n",
       " 'su': 509,\n",
       " 'sv': 510,\n",
       " 'sw': 511,\n",
       " 'sx': 512,\n",
       " 'sy': 513,\n",
       " 'sz': 514,\n",
       " 't': 515,\n",
       " 'ta': 516,\n",
       " 'tb': 517,\n",
       " 'tc': 518,\n",
       " 'td': 519,\n",
       " 'te': 520,\n",
       " 'tf': 521,\n",
       " 'tg': 522,\n",
       " 'th': 523,\n",
       " 'ti': 524,\n",
       " 'tj': 525,\n",
       " 'tk': 526,\n",
       " 'tl': 527,\n",
       " 'tm': 528,\n",
       " 'tn': 529,\n",
       " 'to': 530,\n",
       " 'tp': 531,\n",
       " 'tq': 532,\n",
       " 'tr': 533,\n",
       " 'ts': 534,\n",
       " 'tt': 535,\n",
       " 'tu': 536,\n",
       " 'tv': 537,\n",
       " 'tw': 538,\n",
       " 'tx': 539,\n",
       " 'ty': 540,\n",
       " 'tz': 541,\n",
       " 'u': 542,\n",
       " 'ua': 543,\n",
       " 'ub': 544,\n",
       " 'uc': 545,\n",
       " 'ud': 546,\n",
       " 'ue': 547,\n",
       " 'uf': 548,\n",
       " 'ug': 549,\n",
       " 'uh': 550,\n",
       " 'ui': 551,\n",
       " 'uj': 552,\n",
       " 'uk': 553,\n",
       " 'ul': 554,\n",
       " 'um': 555,\n",
       " 'un': 556,\n",
       " 'uo': 557,\n",
       " 'up': 558,\n",
       " 'uq': 559,\n",
       " 'ur': 560,\n",
       " 'us': 561,\n",
       " 'ut': 562,\n",
       " 'uu': 563,\n",
       " 'uv': 564,\n",
       " 'uw': 565,\n",
       " 'ux': 566,\n",
       " 'uy': 567,\n",
       " 'uz': 568,\n",
       " 'v': 569,\n",
       " 'va': 570,\n",
       " 'vb': 571,\n",
       " 'vc': 572,\n",
       " 'vd': 573,\n",
       " 've': 574,\n",
       " 'vf': 575,\n",
       " 'vg': 576,\n",
       " 'vh': 577,\n",
       " 'vi': 578,\n",
       " 'vj': 579,\n",
       " 'vk': 580,\n",
       " 'vl': 581,\n",
       " 'vm': 582,\n",
       " 'vn': 583,\n",
       " 'vo': 584,\n",
       " 'vp': 585,\n",
       " 'vq': 586,\n",
       " 'vr': 587,\n",
       " 'vs': 588,\n",
       " 'vt': 589,\n",
       " 'vu': 590,\n",
       " 'vv': 591,\n",
       " 'vw': 592,\n",
       " 'vx': 593,\n",
       " 'vy': 594,\n",
       " 'vz': 595,\n",
       " 'w': 596,\n",
       " 'wa': 597,\n",
       " 'wb': 598,\n",
       " 'wc': 599,\n",
       " 'wd': 600,\n",
       " 'we': 601,\n",
       " 'wf': 602,\n",
       " 'wg': 603,\n",
       " 'wh': 604,\n",
       " 'wi': 605,\n",
       " 'wj': 606,\n",
       " 'wk': 607,\n",
       " 'wl': 608,\n",
       " 'wm': 609,\n",
       " 'wn': 610,\n",
       " 'wo': 611,\n",
       " 'wp': 612,\n",
       " 'wq': 613,\n",
       " 'wr': 614,\n",
       " 'ws': 615,\n",
       " 'wt': 616,\n",
       " 'wu': 617,\n",
       " 'wv': 618,\n",
       " 'ww': 619,\n",
       " 'wx': 620,\n",
       " 'wy': 621,\n",
       " 'wz': 622,\n",
       " 'x': 623,\n",
       " 'xa': 624,\n",
       " 'xb': 625,\n",
       " 'xc': 626,\n",
       " 'xd': 627,\n",
       " 'xe': 628,\n",
       " 'xf': 629,\n",
       " 'xg': 630,\n",
       " 'xh': 631,\n",
       " 'xi': 632,\n",
       " 'xj': 633,\n",
       " 'xk': 634,\n",
       " 'xl': 635,\n",
       " 'xm': 636,\n",
       " 'xn': 637,\n",
       " 'xo': 638,\n",
       " 'xp': 639,\n",
       " 'xq': 640,\n",
       " 'xr': 641,\n",
       " 'xs': 642,\n",
       " 'xt': 643,\n",
       " 'xu': 644,\n",
       " 'xv': 645,\n",
       " 'xw': 646,\n",
       " 'xx': 647,\n",
       " 'xy': 648,\n",
       " 'xz': 649,\n",
       " 'y': 650,\n",
       " 'ya': 651,\n",
       " 'yb': 652,\n",
       " 'yc': 653,\n",
       " 'yd': 654,\n",
       " 'ye': 655,\n",
       " 'yf': 656,\n",
       " 'yg': 657,\n",
       " 'yh': 658,\n",
       " 'yi': 659,\n",
       " 'yj': 660,\n",
       " 'yk': 661,\n",
       " 'yl': 662,\n",
       " 'ym': 663,\n",
       " 'yn': 664,\n",
       " 'yo': 665,\n",
       " 'yp': 666,\n",
       " 'yq': 667,\n",
       " 'yr': 668,\n",
       " 'ys': 669,\n",
       " 'yt': 670,\n",
       " 'yu': 671,\n",
       " 'yv': 672,\n",
       " 'yw': 673,\n",
       " 'yx': 674,\n",
       " 'yy': 675,\n",
       " 'yz': 676,\n",
       " 'z': 677,\n",
       " 'za': 678,\n",
       " 'zb': 679,\n",
       " 'zc': 680,\n",
       " 'zd': 681,\n",
       " 'ze': 682,\n",
       " 'zf': 683,\n",
       " 'zg': 684,\n",
       " 'zh': 685,\n",
       " 'zi': 686,\n",
       " 'zj': 687,\n",
       " 'zk': 688,\n",
       " 'zl': 689,\n",
       " 'zm': 690,\n",
       " 'zn': 691,\n",
       " 'zo': 692,\n",
       " 'zp': 693,\n",
       " 'zq': 694,\n",
       " 'zr': 695,\n",
       " 'zs': 696,\n",
       " 'zt': 697,\n",
       " 'zu': 698,\n",
       " 'zv': 699,\n",
       " 'zw': 700,\n",
       " 'zx': 701,\n",
       " 'zy': 702,\n",
       " 'zz': 703}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tok = tokenizer(vocabulary)\n",
    "\n",
    "ngram_to_id = {char: idx for idx, char in enumerate(tok)}\n",
    "ngram_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 650, 512])\n",
      "torch.Size([16, 650, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.blocks import MHSA, FFN, PrenormEncoder\n",
    "\n",
    "seq_len = 650\n",
    "emb_dim = 512\n",
    "batch = 16\n",
    "#num_heads = 4\n",
    "\n",
    "x = torch.randn(batch, seq_len, emb_dim)\n",
    "\n",
    "#mhsa = MHSA(emb_dim, num_heads)\n",
    "#ffn = FFN(emb_dim, 0.2)\n",
    "encoder = PrenormEncoder(emb_dim, 4, 0.2)\n",
    "\n",
    "print(f'{x.shape}')\n",
    "out = encoder(x)\n",
    "print(f'{out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiGRU exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 650, 512])\n",
      "torch.Size([16, 650, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.blocks import BiGRU\n",
    "\n",
    "seq_len = 650\n",
    "emb_dim = 512\n",
    "batch_size = 16\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, emb_dim)\n",
    "\n",
    "bigru = BiGRU(emb_dim, emb_dim, 0.2, True)\n",
    "\n",
    "print(f'{x.shape}')\n",
    "out = bigru(x)\n",
    "print(f'{out.shape}') # emb_dim = emb_dim * 2 (because it is bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 64, 650])\n",
      "torch.Size([16, 32, 64, 650])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.blocks import ResNet\n",
    "\n",
    "batch_size = 16\n",
    "channel = 32\n",
    "time = 650\n",
    "freq = 64\n",
    "\n",
    "x = torch.randn(batch_size, channel, freq, time)\n",
    "# resnet = ResNet(channel, channel, 3, 1, 0.2, freq)\n",
    "\n",
    "# print(f'{x.shape}')\n",
    "# out = resnet(x)\n",
    "#print(f'{out.shape}')\n",
    "\n",
    "layer_norm = nn.LayerNorm(time)\n",
    "\n",
    "print(f'{x.shape}')\n",
    "out = layer_norm(x)\n",
    "print(f'{out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Jasper from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.jasper.blocks import InBlock, OutBlock, Encoder, Decoder\n",
    "\n",
    "n_features = 128\n",
    "seq_len = 600\n",
    "x = torch.randn(batch_size, n_features, seq_len).to(device)\n",
    "\n",
    "inblock = InBlock(in_channels=n_features,\n",
    "                  out_channels=n_features*2,\n",
    "                  kernel_size=11,\n",
    "                  stride=2,\n",
    "                  padding=1,\n",
    "                  drop_rate=0.2).to(device)\n",
    "\n",
    "'''\n",
    "outblock = OutBlock(in_channels=n_features,\n",
    "                    out_channels=n_features*2,\n",
    "                    n_blocks=3,\n",
    "                    kernel_size=11,\n",
    "                    drop_rate=0.2).to(device)\n",
    "'''\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "x = inblock(x)\n",
    "x = encoder(x)\n",
    "x = decoder(x)\n",
    "print(f'{x.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
