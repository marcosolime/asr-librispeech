{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep SpeechüèãÔ∏è\n",
    "- in this notebook we train Deep Speech-based models\n",
    "- we vary the number of conv and res blocks and check their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/5 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom libraries\n",
    "from utils.preprocessing import Preprocessing\n",
    "from utils.word_model import WordModel\n",
    "from utils.decoders import DecoderBase\n",
    "from utils.metrics import avg_cer, avg_wer\n",
    "from utils.misc import pretty_params\n",
    "\n",
    "# Plots\n",
    "import wandb\n",
    "\n",
    "# Models\n",
    "from models.deep_speech_base import DeepSpeechBase\n",
    "from models.deep_speech_attention import DeepSpeechAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- if you set `word_model` to `unigram`, set `stride=2`\n",
    "- if you set `word_model` to `bigram`, set `stride=2`\n",
    "- striding on the frequencies is set to `2` (left unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "seed = 42\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "# Unigram: apostrophe (+1), space (+1), alphabet (+26), blank (+1) = 29\n",
    "# Bigram: apostrophe (+1), space (+1), alphabet (+702), blank (+1) = 705\n",
    "#word_model = WordModel(\"unigram\")\n",
    "word_model = WordModel(\"bigram\")\n",
    "\n",
    "n_features = 128\n",
    "stride = 4              # time-axis striding\n",
    "\n",
    "stage_1 = 3                 # 1st stage\n",
    "stage_2 = 6                 # 2nd stage\n",
    "\n",
    "rnn_dim = 512           # for DeepSpeechBase\n",
    "emb_dim = 512           # for DeepSpeechAttention\n",
    "\n",
    "drop_rate = 0.2\n",
    "lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up CUDA device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/5 Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\.conda\\envs\\exp\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data processing using custom library\n",
    "prep = Preprocessing()\n",
    "train_set = prep.download(split='train', download=False)\n",
    "# dev test\n",
    "test_set = prep.download(split='test', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: prep.preprocess(x, \"train\", stride, word_model))\n",
    "\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: prep.preprocess(x, \"test\", stride, word_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~ TRAIN ~\n",
      "Number of batches: 1784\n",
      "Number of samples: 28539\n",
      "\n",
      "~ TEST ~\n",
      "Number of batches: 164\n",
      "Number of samples: 2620\n",
      "\n",
      "+------ Dataloader length: 28539 ------+\n",
      "# Batches: 1784\n",
      "Spectogram shape: [16, 1, 128, 1308]\n",
      "Label shape: [16, 164]\n",
      "Mel length (length of each spectogram): [288, 327, 150, 242, 271, 292] ...\n",
      "Idx length (length of each label): [123, 164, 81, 110, 128, 130] ...\n",
      "+------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "n_batches_train = int(len(train_loader))\n",
    "n_samples_train = int(len(train_loader.dataset))\n",
    "\n",
    "n_batches_test = int(len(test_loader))\n",
    "n_samples_test = int(len(test_loader.dataset))\n",
    "\n",
    "print(\"~ TRAIN ~\")\n",
    "print(f\"Number of batches: {n_batches_train}\")\n",
    "print(f\"Number of samples: {n_samples_train}\")\n",
    "print()\n",
    "\n",
    "print(\"~ TEST ~\")\n",
    "print(f\"Number of batches: {n_batches_test}\")\n",
    "print(f\"Number of samples: {n_samples_test}\")\n",
    "print()\n",
    "\n",
    "prep.print_loader_info(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/5 Model\n",
    "- there are 2 available models: DeepSpeechBase and DeepSpeechAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 28.78M (28778945)\n"
     ]
    }
   ],
   "source": [
    "# DeepSpeechBase\n",
    "deep_speech = DeepSpeechBase(stage_1, \n",
    "                             stage_2, \n",
    "                             rnn_dim, \n",
    "                             n_features, \n",
    "                             word_model.get_n_class(),\n",
    "                             stride=stride,\n",
    "                             drop_rate=drop_rate).to(device)\n",
    "\n",
    "tot_params = sum([p.numel() for p in deep_speech.parameters()])\n",
    "model_name=\"deep_speech_base\"\n",
    "print(f\"Number of parameters: {pretty_params(tot_params)} ({tot_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 11.20M (11197889)\n"
     ]
    }
   ],
   "source": [
    "# DeepSpeechAttention\n",
    "deep_speech = DeepSpeechAttention(stage_1,\n",
    "                                  stage_2,\n",
    "                                  n_features,\n",
    "                                  word_model.get_n_class(),\n",
    "                                  emb_dim,\n",
    "                                  n_heads=4,\n",
    "                                  stride=stride,\n",
    "                                  drop_rate=0.2).to(device)\n",
    "\n",
    "tot_params = sum([p.numel() for p in deep_speech.parameters()])\n",
    "model_name=\"deep_speech_attention\"\n",
    "print(f\"Number of parameters: {pretty_params(tot_params)} ({tot_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "path_to_weights = \"./weights/deep_speech_12_04.pth\"\n",
    "deep_speech.load_state_dict(torch.load(path_to_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder \n",
    "decoder = DecoderBase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/5 Optimizer, loss, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, loss, scheduler\n",
    "adamW = optim.AdamW(deep_speech.parameters(), lr)\n",
    "ctc_loss = nn.CTCLoss(blank=word_model.get_blank_id()).to(device)\n",
    "one_cycle_lr = optim.lr_scheduler.OneCycleLR(adamW,\n",
    "                                             max_lr=lr,\n",
    "                                             steps_per_epoch=n_batches_train,\n",
    "                                             epochs=epochs,\n",
    "                                             anneal_strategy=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5/5 Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: iu4hry (fantastic_4). Use `wandb login --relogin` to force relogin\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miu4hry\u001b[0m (\u001b[33mfantastic_4\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\marco\\OneDrive - Alma Mater Studiorum Universit√† di Bologna\\University\\NLP_Torroni\\ASR\\asr-librispeech\\wandb\\run-20240504_120154-wzgalslm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fantastic_4/asr_librispeech/runs/wzgalslm/workspace' target=\"_blank\">jedi-carrier-22</a></strong> to <a href='https://wandb.ai/fantastic_4/asr_librispeech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fantastic_4/asr_librispeech' target=\"_blank\">https://wandb.ai/fantastic_4/asr_librispeech</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fantastic_4/asr_librispeech/runs/wzgalslm/workspace' target=\"_blank\">https://wandb.ai/fantastic_4/asr_librispeech/runs/wzgalslm/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fantastic_4/asr_librispeech/runs/wzgalslm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x271ff000530>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Online fancy plots\n",
    "! wandb login\n",
    "\n",
    "wandb.init(\n",
    "    project=\"asr_librispeech\",\n",
    "\n",
    "    config= {\n",
    "        \"model\": model_name,\n",
    "        \"stage_1\": stage_1,\n",
    "        \"stage_2\": stage_2,\n",
    "        \"word_model\": word_model.get_name(),\n",
    "        \"stride\": stride\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "def train(epoch, dataset_loader, model, optimizer, scheduler, fn_loss):\n",
    "    print(f\"Traininig... (e={epoch})\")\n",
    "    \n",
    "    # Train mode ON\n",
    "    model.train()\n",
    "    n_samples = int(len(dataset_loader.dataset))\n",
    "\n",
    "    for idx, audio_data in enumerate(dataset_loader):\n",
    "        \n",
    "        # Get audio data\n",
    "        spectograms, indices, len_spectograms, len_indices = audio_data\n",
    "        spectograms, indices = spectograms.to(device), indices.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(spectograms) # [batch, time, n_class]\n",
    "        out = F.log_softmax(out, dim=2)\n",
    "        out = out.transpose(0, 1) # [time, batch, n_class]\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = fn_loss(out, indices, len_spectograms, len_indices)\n",
    "        loss.backward()\n",
    "\n",
    "        # Log\n",
    "        wandb.log({\n",
    "            \"loss\": loss.item(),\n",
    "            \"lr\": scheduler.get_last_lr()\n",
    "        })\n",
    "\n",
    "        # Step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log\n",
    "        if idx % 10 == 0 or idx == n_samples:\n",
    "            print(\"Epoch: {}, [{}/{}], Loss: {:.6f}\".format(\n",
    "                epoch, \n",
    "                idx*len(spectograms), \n",
    "                n_samples,\n",
    "                loss.item()))\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, dataset_loader, model, optimizer, fn_loss, debug=False):\n",
    "    print(f\"Testing... (e={epoch})\")\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    wer_list = []\n",
    "    cer_list = []\n",
    "\n",
    "    n_batch = int(len(dataset_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, audio_data in enumerate(dataset_loader):\n",
    "        \n",
    "            # Get audio data\n",
    "            spectograms, indices, len_spectograms, len_indices = audio_data\n",
    "            spectograms, indices = spectograms.to(device), indices.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(spectograms) # [batch, time, n_class]\n",
    "            out = F.log_softmax(out, dim=2)\n",
    "            out = out.transpose(0, 1) # [time, batch, n_class]\n",
    "\n",
    "            # Compute loss (but do not backprop)\n",
    "            loss = fn_loss(out, indices, len_spectograms, len_indices)\n",
    "            total_loss += loss.item() / n_batch\n",
    "\n",
    "            # Metrics\n",
    "            decode_hypothesis = decoder.decode_prob(out, word_model)\n",
    "            decode_reference = decoder.decode_labels(indices, len_indices, word_model)\n",
    "\n",
    "            wer_list.append(avg_wer(decode_hypothesis, decode_reference))\n",
    "            cer_list.append(avg_cer(decode_hypothesis, decode_reference))\n",
    "            \n",
    "            if debug: break\n",
    "            \n",
    "    print(f\"Loss: {total_loss:.6f}\")\n",
    "    print(f\"WER: {sum(wer_list)/len(wer_list):.4f}\")\n",
    "    print(f\"CER: {sum(cer_list)/len(cer_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful: this will kill you GPU\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch, train_loader, deep_speech, adamW, one_cycle_lr, ctc_loss)\n",
    "    test(epoch, test_loader, deep_speech, adamW, ctc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy train (one epoch)\n",
    "train(1, train_loader, deep_speech, adamW, one_cycle_lr, ctc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test(1, test_loader, deep_speech, adamW, ctc_loss, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
