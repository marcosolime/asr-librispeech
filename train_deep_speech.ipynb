{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep SpeechüèãÔ∏è\n",
    "- in this notebook we train Deep Speech-based models\n",
    "- we vary the number of conv and res blocks and check their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/5 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom libraries\n",
    "from utils.preprocessing import Preprocessing\n",
    "from utils.decoders import DecoderBase\n",
    "from utils.metrics import batch_cer, batch_wer\n",
    "from utils.misc import pretty_params\n",
    "\n",
    "# Plots\n",
    "import wandb\n",
    "\n",
    "# Models\n",
    "from models.deep_speech import DeepSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "seed = 42\n",
    "batch_size = 16\n",
    "blank_id = 28\n",
    "epochs = 10\n",
    "n_class = 29            # alphabet (+26), space (+1), apostrophe (+1), blank (+1) = 29\n",
    "\n",
    "n_features = 128\n",
    "stride = 2              # time-freq downsample\n",
    "n_conv2d = 6            # 1st stage\n",
    "n_rnns = 3              # 2nd stage\n",
    "rnn_dim = 512\n",
    "drop_rate = 0.1\n",
    "lr = 0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up CUDA device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/5 Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\.conda\\envs\\exp\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data processing using custom library\n",
    "prep = Preprocessing()\n",
    "train_set = prep.download(split='train', download=False)\n",
    "test_set = prep.download(split='test', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: prep.preprocess(x, \"train\"))\n",
    "\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: prep.preprocess(x, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~ TRAIN ~\n",
      "Number of batches: 1784\n",
      "Number of samples: 28539\n",
      "\n",
      "~ TEST ~\n",
      "Number of batches: 164\n",
      "Number of samples: 2620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_batches_train = int(len(train_loader))\n",
    "n_samples_train = int(len(train_loader.dataset))\n",
    "\n",
    "n_batches_test = int(len(test_loader))\n",
    "n_samples_test = int(len(test_loader.dataset))\n",
    "\n",
    "print(\"~ TRAIN ~\")\n",
    "print(f\"Number of batches: {n_batches_train}\")\n",
    "print(f\"Number of samples: {n_samples_train}\")\n",
    "print()\n",
    "\n",
    "print(\"~ TEST ~\")\n",
    "print(f\"Number of batches: {n_batches_test}\")\n",
    "print(f\"Number of samples: {n_samples_test}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 14.25M (14252829)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "deep_speech = DeepSpeech(n_conv2d, n_rnns, rnn_dim, n_class, n_features).to(device)\n",
    "\n",
    "tot_params = sum([p.numel() for p in deep_speech.parameters()])\n",
    "print(f\"Number of parameters: {pretty_params(tot_params)} ({tot_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder \n",
    "decoder = DecoderBase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/5 Optimizer, loss, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, loss, scheduler\n",
    "optimizer = optim.AdamW(deep_speech.parameters(), lr)\n",
    "criterion = nn.CTCLoss(blank=blank_id).to(device)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                          max_lr=lr,\n",
    "                                          steps_per_epoch=n_batches_train,\n",
    "                                          epochs=epochs,\n",
    "                                          anneal_strategy=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5/5 Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online fancy plots\n",
    "! wandb login\n",
    "\n",
    "wandb.init(\n",
    "    project=\"asr_librispeech\",\n",
    "\n",
    "    config= {\n",
    "        \"model\": \"deep_speech\",\n",
    "        \"n_conv2d\": n_conv2d,\n",
    "        \"n_rnns\": n_rnns,\n",
    "        \"rnn_dim\": rnn_dim\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "def train(epoch):\n",
    "    print(f\"Traininig... (e={epoch})\")\n",
    "    \n",
    "    # Train mode ON\n",
    "    deep_speech.train()\n",
    "    \n",
    "    for idx, audio_data in enumerate(train_loader):\n",
    "        \n",
    "        # Get audio data\n",
    "        spectograms, indices, len_spectograms, len_indices = audio_data\n",
    "        spectograms, indices = spectograms.to(device), indices.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = deep_speech(spectograms) # [batch, time, n_class]\n",
    "        out = F.log_softmax(out, dim=2)\n",
    "        out = out.transpose(0, 1) # [time, batch, n_class]\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = criterion(out, indices, len_spectograms, len_indices)\n",
    "        loss.backward()\n",
    "\n",
    "        # Log\n",
    "        wandb.log({\n",
    "            \"loss\": loss.item(),\n",
    "            \"lr\": scheduler.get_last_lr()\n",
    "        })\n",
    "\n",
    "        # Step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log\n",
    "        if idx % 10 == 0 or idx == n_samples_train:\n",
    "            # avg_wer = sum(wer_list) / len(wer_list)\n",
    "            # avg_cer = sum(cer_list) / len(cer_list)\n",
    "            print(\"Epoch: {}, [{}/{}], Loss: {:.6f}\".format(\n",
    "                epoch, \n",
    "                idx*len(spectograms), \n",
    "                n_samples_train,\n",
    "                loss.item()))\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    print(f\"Testing... (e={epoch})\")\n",
    "    deep_speech.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    wer_list = []\n",
    "    cer_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, audio_data in enumerate(train_loader):\n",
    "        \n",
    "            # Get audio data\n",
    "            spectograms, indices, len_spectograms, len_indices = audio_data\n",
    "            spectograms, indices = spectograms.to(device), indices.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = deep_speech(spectograms) # [batch, time, n_class]\n",
    "            out = F.log_softmax(out, dim=2)\n",
    "            out = out.transpose(0, 1) # [time, batch, n_class]\n",
    "\n",
    "            # Compute loss (but do not backprop)\n",
    "            loss = criterion(out, indices, len_spectograms, len_indices)\n",
    "            total_loss += loss.item() / n_batches_test\n",
    "\n",
    "            # Metrics\n",
    "            decode_preds = decoder.decode_prob(out, prep.tokenizer)\n",
    "            decode_targets = decoder.decode_labels(indices, len_indices, prep.tokenizer)\n",
    "\n",
    "            wer_list.append(batch_wer(decode_targets, decode_preds, average=True))\n",
    "            cer_list.append(batch_cer(decode_targets, decode_preds, average=True))\n",
    "\n",
    "    print(f\"Loss: {total_loss:.6f}\")\n",
    "    print(f\"WER: {sum(wer_list)/len(wer_list):.4f}\")\n",
    "    print(f\"CER: {sum(cer_list)/len(cer_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful: this will kill you GPU\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
